- operating system is stored in the secondary memory

-> Types of OS:
	1) Batch:
		- similar types of jobs executed together
		- the jobs were loaded offline into punchcards/magnetic tapes.
	2) Multiprogrammed * :
		- Bring as many processes possible into RAM to increase CPU utilization
		- usually non-preemptive execution, which is not very effective in case of I/O operations.
	3) Multitasking/Time Sharing *
		- preemptive, hence more responsive
		- this is what is used more often
	4) Real time OS:
		- there's strict time constraints, there should be no delays.
	5) Distributed:
		- environments are distributed, not the OS. all are connected through some network.
		- geographically separated, each may have their own processing capacities.
	6) Clustered:
		- connected locally in one network.
		- 100 machines working as 1 machine
		- load balancing and scalability is good
	7) Embedded:
		- work on a fixed functionality
		- like microwave, washing machine, etc.

-> Goals of an OS:
	1) Abstraction: Hide the details of underlying hardware from the user app
	
	2) Resource management: Virtualization to increase efficiency
		- Virtual CPUs: very fast preemption that gives the illusion to each process that the entire CPU is allocated to them.
		- Virtual Memory: allows to store the processes in chunks in memory (distributed, not contiguous) that allows for multiple processes to be stored. the mappings are maintained using a virtual memory map, and the CPU accesses these logical addresses. these are converted to physical addresses by the MMU using OS.
		
	3) Isolation: Dual mode of operation. Demarcate the region of memory reserved for OS and that for the user. OS part contains "priviledged instructions", user part contains "non-priviledged instructions". there is a H/W mode bit, which when set to 0 indicates execution of PRIVILEDGED, and set to 1 indicates NON-PRIVILEDGED. the priviledged instructions are accessed by making system calls to the OS/kernel.


- for a computer to start running (like when it is switched on), the BOOTSTRAP PROGRAM is the initial program that is run. it is stored inside the ROM or EEPROM (aka firmware). It initializes all aspects of the system, from CPU registers to device controllers to memory contents. The bootstrap program must know how to load the operating system and how to start executing that system. the bootstrap program must locate the operating-system kernel and load it into memory.
the different steps involved when the CPU is turned on are:
	1) Initialization of registers
	2) access to BIOS (Basic Input Output Setup). THis is accessed before the RAM/PM is accessed. there is one register that points to the BIOS. the code in the BIOS is executed first.
		- performs a POST check (Power on Self Test). it checks whether all the devices connected to the CPU are working properly or not.
		- identifies the Boot device (the device that holds the OS binary) to be brought to the PM.
		- reads the first 512 bytes of sector 0 of the HDD (the Master Boot Record, contains information about the data stored in other sectors. the hdd has many discs, each disc has concentric rings called traps, and is divided into multiple arcs called sectors)
		- there can be multiple OS binaries located in the HDD. the MBR should know which to load (the start point of that OS binary to its end point)
		- once the sector for that OS binary is identified, that entire sector is loaded into memory.
	3) Boot Loader Program: the program that actually loads the OS. if there is only one OS, it directly loads that OS into memory. if multiple OS's, it gives the option to the user to choose which OS to load.

- whenever we execute something, it gets loaded onto the main memory (RAM)

- a modern general purpose computer system may consist of one or more CPUs, and a number of device
controllers connected through a common bus that provide access to a shared memory.

- each device controllers is in charge of a specific type of device. cpu and device controllers can execute at the same time, competing for memory cycles. 

- but to ensure orderly access of memory by these controllers, there is a memory controller who's function is to synchronize access to memory.

- when a CPU is interrupted, it stops what it is doing and immediately transfers execution to a fixed location. the fixed location usually contains the starting address of the Service Routine of the interrupt (service routine is basically what the interrupt wants to do).

- so what does an OS do?
	1) Process management
	2) Process scheduling
	3) Inter Process Communication & Synchronization
	4) Memory management
	5) I/O & File management.


#STORAGE
- registers are the smallest storage unit. followed by cache and main memory. as we go down, cost per bit reduces and size of memory increases. speed reduces as well.

- eqach device controller maintains a LOCAL BUFFER STORAGE and a SET OF SPECIAL PURPOSE REGISTERS (SPRs).

- each device controller also has a device driver that acts like a middleware for the communication between the OS and the device controller.

---> Working of an I/O operation:

 
- Once the kernel is loaded and executing, it can start providing services to the system and its users. Some services are provided outside of the kernel, by SYSTEM PROGRAMS that are loaded into memory at boot time to become SYSTEM PROCESSES, or SYSTEM DAEMONS that run the entire time the kernel is running. On unix, the first system process to be run is the "init" process, that starts many other daemons. Once this phase is complete, the system is fully booted, and the system waits for some event to occur.

- The occurrence of an event is usually signaled by an interrupt from either the hardware or the software. Hardware may trigger an interrupt at any time by sending a signal to the CPU, usually by way of the system bus. Software may trigger an interrupt by executing a special operation called a SYSTEM CALL (also called a MONITOR CALL).

- When the CPU is interrupted, it stops what it is doing and immediately transfers execution to a fixed location. The fixed location usually contains the starting address where the service routine for the interrupt is located. The interrupt service routine executes; on completion, the CPU resumes the interrupted computation.

- Since only a predefined number of interrupts is possible, a table of pointers to interrupt routines can be used instead to provide the necessary speed. The interrupt routine is called indirectly through the table, with no intermediate routine needed. Generally, the table of pointers is stored in low memory (the first hundred or so locations). These locations hold the addresses of the interrupt service routines for the various devices.

- The interrupt architecture must also save the address of the interrupted instruction. More recent architectures store the return address on the SYSTEM STACK. After the interrupt is serviced, the saved return address is loaded into the program counter, and the interrupted computation resumes as though the interrupt had not occurred.

- Interaction is achieved through a sequence of load or store instructions to specific memory addresses. The load instruction moves a byte or word from main memory to an internal register within the CPU, whereas the store instruction moves the content of a register to main memory. Aside from explicit loads and stores, the CPU automatically loads instructions from main memory for execution.

- A general-purpose computer system consists of CPUs and multiple device controllers that are connected through a common bus. Each device controller is in charge of a specific type of device. A device controller maintains some local buffer storage and a set of special-purpose registers. The device controller is responsible for moving the data between the peripheral devices that it controls and its local buffer storage. operating systems have a device driver for each device controller. This device driver understands the device controller and provides the rest of the operating system with a uniform interface to the device.

									OS <--- Device Driver <--- Device Controller <--- Similar Devices

- device driver is sort of a middleware that helps in managing read/write operations between devices, and control flow of information between the devices and the respective device driver.

1) the device driver loads the appropriate registers within the device controller.
2) the device controller examines the loaded registers to know what action to take.
3) the controller then starts the transfer of data to-and-from the device and its local buffer.
4) once transfer of data is done, the device controller informs the device driver via an interrrupt that operation is over.
5) the device driver then returns control to the OS.



- this is good only for small amounts of data. can produce high overhead for larger data. prefer to use DMA (Direct Memory Access) for larger data, whereby the device controller transfers an etnire block of data directly to-or-fro its own buffer storage to memory, with no intervention by the CPU.

- in DMA, 1) to 3) same. but when returning control to the OS, it uses the above approach which avoids overhead.

- in non-DMA, one interrupt for each byte of data (too many interrupts as a result). in DMA, one interrupt for a block of data. as a result, the CPU can utilize the time to perform other works as well as only one interrupt is generated in DMA when the operation is completed.



# COMPUTER SYSTEM ARCHITECTURE
- two types of systems based on number of processors: single processor and multiprocessor systems (names are self explanatory).

1) SINGLE PROCESSOR SYSTEMS:

- On a single-processor system, there is one main CPU capable of executing a general-purpose instruction set, including instructions from user processes. Almost all single- processor systems have other special-purpose processors as well.

- All of these special-purpose processors run a limited instruction set and do not run user processes. Sometimes, they are managed by the operating system, in that the operating system sends them information about their next task and monitors their status. For example, a disk-controller microprocessor receives a sequence of requests from the main CPU and implements its own disk queue and scheduling algorithm. This arrangement relieves the main CPU of the overhead of disk scheduling. PCs contain a microprocessor in the keyboard to convert the keystrokes into codes to be sent to the CPU. In other systems or circumstances, special-purpose processors are low-level components built into the hardware. The operating system cannot communicate with these processors; they do their jobs autonomously. The use of special-purpose microprocessors does not turn the single-processor system into a multi-processor system. As long as there is just one single general purpose CPU, it is a single-processor system.

2) MULTIPLE PROCESSOR SYSTEMS:

- multiprocessor systems (also known as parallel systems or multicore systems) have begun to dominate the landscape of computing. Such systems have two or more processors in close communication, sharing the computer bus and sometimes the clock, memory, and peripheral devices. main advantages are:
	1) increased throughput: get more work done in less time. The speed-up ratio with N processors is not N, however; rather, it is less than N. When multiple processors cooperate on a task, a certain amount of overhead is incurred in keeping all the parts working correctly. This overhead, plus contention for shared resources, lowers the expected gain from additional processors.
	2) cheaper: it is cheaper to have a single system with multiple processors to manage and share I/O operations, disk operations, memory, etc. than to have multiple single processor systems with local disks and memory and local copies of data.
	3) reliable: If functions can be distributed properly among several processors, then the failure of one processor will not halt the system, only slow it down. If we have ten processors and one fails, then each of the remaining nine processors can pick up a share of the work of the failed processor. Thus, the entire system runs only 10 percent slower, rather than failing altogether.
	
- The ability to continue providing service proportional to the level of surviving hardware is called GRACEFUL DEGRADATION. Some systems go beyond graceful degradation and are called FAULT TOLERANT, because they can suffer a failure of any single component and still continue operation.

- Some systems use asymmetric multiprocessing, in which each processor is assigned a specific task. A boss processor controls the system; the other processors either look to the boss for instruction or have predefined tasks. This scheme defines a boss–worker relationship. The boss processor schedules and allocates work to the worker processors.

- The most common systems use symmetric multiprocessing (SMP), in which each processor performs all tasks within the operating system. SMP means that all processors are peers; no boss–worker relationship exists between processors.

- we must carefully control I/O to ensure that the data reach the appropriate processor. Also, since the CPUs are separate, one may be sitting idle while another is overloaded, resulting in inefficiencies. These inefficiencies can be avoided if the processors share certain data structures.

- multiprocessing can cause a system to change its memory access model from uniform memory access (UMA) to non-uniform memory access (NUMA). UMA is defined as the situation in which access to any RAM from any CPU takes the same amount of time. With NUMA, some parts of memory may take longer to access than other parts, creating a performance penalty.

- A recent trend in CPU design is to include multiple computing cores on a single chip. Such multiprocessor systems are termed multicore. They can be more efficient than multiple chips with single cores because on-chip communication is faster than between-chip communication. In addition, one chip with multiple cores uses significantly less power than multiple single-core chips.

- multicore systems are multiprocessor systems, not all multiprocessor systems are multicore

- Each core has its own register set as well as its own local cache. Other designs might use a shared cache or a combination of local and shared caches. Aside from architectural considerations, such as cache, memory, and bus contention, these multicore CPUs appear to the operating system as N standard processors.

- single processor can have multiple special purpose processors for different devices, multiprocessors are more reliable, economical, and increase throughput.


- multiprocessor systems can be of two types: SMP (Symmetric MultiProcessing) and AMP (Asymmetric MultiProcessing) and Clustered Systems (further into Symmetric and Asymmetric).

- The set of jobs in memory can be a subset of the jobs kept in the job pool. The operating system picks and begins to execute one of the jobs in main memory. Eventually, the job may have to wait for some task, such as an I/O operation, to complete. In a non-multiprogrammed system, the CPU would sit idle. In a multiprogrammed system, the operating system simply switches to, and executes, another job. When that job needs to wait, the CPU switches to another job, and so on. Eventually, the first job finishes waiting and gets the CPU back. As long as at least one job needs to execute, the CPU is never idle.

- An operating system MUST be capable of providing these two features:
	1) Multiprogramming: the CPU should not wait for a task to finish (let's say, I/O) before starting another task. multiprogramming increases CPU utilization by organizing jobs so that the CPU is always executing a task no matter what.
		- all jobs exist in the job pool initially. the jobs that are brought into the main memory are a subset of the jobs in the job pool. jobs are then picked from the memory for execution by the CPU. 
		
	2) Time Sharing (Multitasking): 
		- CPU executes multiple jobs by switching among them. these switches occur so frequently that the user can interact with each program while it is running. (user can interact even when it is running, unlike in multiprogramming).
		
		- this allows many users to feel as though only they are using the resources, as the job switching/CPU scheduling occurs very quickly using in a time shared system. 
		
		- a program loaded into memory and waiting execution by the CPU is called a process.
		
- Time sharing (or multitasking) is a logical extension of multiprogramming. In time-sharing systems, the CPU executes multiple jobs by switching among them, but the switches occur so frequently that the users can interact with each program while it is running.

- A time-shared operating system allows many users to share the computer simultaneously. Since each action or command in a time-shared system tends to be short, only a little CPU time is needed for each user. As the system switches rapidly from one user to the next, each user is given the impression that the entire computer system is dedicated to his use, even though it is being shared among many users.

- A time-shared operating system uses CPU scheduling and multiprogramming to provide each user with a small portion of a time-shared computer. Each user has at least one separate program in memory. A program loaded into memory and executing is called a process.

- When a process executes, it typically executes for only a short time before it either finishes or needs to perform I/O. I/O may be interactive; that is, output goes to a display for the user, and input comes from a user keyboard, mouse, or other device. Since interactive I/O typically runs at “people speeds,” it may take a long time to complete. Rather than let the CPU sit idle as this interactive input takes place, the operating system will rapidly switch the CPU to the program of some other user.

- Time sharing and multiprogramming require that several jobs be kept simultaneously in memory. If several jobs are ready to be brought into memory, and if there is not enough room for all of them, then the system must choose among them. Making this decision involves job scheduling/process scheduling.

- In addition, if several jobs are ready to run at the same time, the system must choose which job will run first. Making this decision is CPU scheduling.

- The job scheduling is known as the long-term scheduling while the CPU scheduling is known as the short-term scheduling. The algorithms like FCFS, SJF, LRTF, etc. are all CPU scheduling. The job scheduling / process scheduling is the mechanism to select processes from this storage and to bring them into the ready queue
	
	
		
-> OPERATING SYSTEM SERVICES:
1) User Interface
2) Program Execution
3) I/O Operations
4) File Sysetm manipulation
5) Communication
6) Error Detection
7) Resources Allocation
8) Accounting
9) Protection and Security.

# SYSTEM CALLS:
	-> Types:
		FILE RELATED: open(), read(), write(), close(), create_file(), etc.
		DEVICE RELATED: read(), write(), reposition(), ioctl(), fcntl(), etc.
		INFORMATION: get_pid(), get_process_attribute(), get_system_time_and_data(), etc.
		PROCESS CONTROL: load(), execute(), abort(), fork(), wait(), signal(), allocate(), etc.
		COMMUNICATION: pipe(), create/delete connections, shmget(), etc.

		- allow us to access the kernel of an operating system when using APIs. the APIs can make system calls that allow the our program to interact with the hardware/OS services (through the kernel). users can only access the user mode of a processor. to interact with devices, the API makes a system call to the kernel, switching to kernel mode (to access the functionalities of the operating system)

		- System calls provide an interface to the services made available by an operating system. These calls are generally available as routines written in C and C++, although certain low-level tasks (for example, tasks where hardware must be accessed directly) may have to be written using assembly-language instructions.

		- even simple programs may make heavy use of the operating system. Frequently, systems execute thousands of system calls per second. Most programmers never see this level of detail, however.

		- Typically, application developers design programs according to an application programming interface (API). The API specifies a set of functions that are available to an application programmer, including the parameters that are passed to each function and the return values the programmer can expect. A programmer accesses an API via a library of code provided by the operating system. In the case of UNIX and Linux for programs written in the C language, the library is called libc. Each operating system has its own name for each system call.

		- Behind the scenes, the functions that make up an API typically invoke the actual system calls on behalf of the application programmer. For example, the Windows function CreateProcess() (which unsurprisingly is used to create a new process) actually invokes the NTCreateProcess() system call in the Windows kernel.

		- For most programming languages, the run-time support system (a set of functions built into libraries included with a compiler) provides a system- call interface that serves as the link to system calls made available by the operating system. The system-call interface intercepts function calls in the API and invokes the necessary system calls within the operating system. Typically, a number is associated with each system call, and the system-call interface maintains a table indexed according to these numbers. The system call interface then invokes the intended system call in the operating-system kernel and returns the status of the system call and any return values.

		- The caller need know nothing about how the system call is implemented or what it does during execution. Rather, the caller need only obey the API and understand what the operating system will do as a result of the execution of that system call. Thus, most of the details of the operating-system interface are hidden from the programmer by the API and are managed by the run-time support library.

		---> TYPES OF SYSTEM CALLS:
		- System calls can be grouped roughly into six major categories: process control, file manipulation, device manipulation, information maintenance, communications, and protection

		1) Process Control: 
			- A running program needs to be able to halt its execution either normally (end()) or abnormally (abort()).
			
			- Under either normal or abnormal circumstances, the operating system must transfer control to the invoking command interpreter (like the shell that calls it, like bash, etc.) The command interpreter then reads the next command. In an interactive system, the command interpreter simply continues with the next command; it is assumed that the user will issue an appropriate command to respond to any error. In a GUI system, a pop-up window might alert the user to the error and ask for guidance.
			
			- A process or job executing one program may want to load() and execute() another program.
			
			- If control returns to the existing program when the new program terminates, we must save the memory image of the existing program; thus, we have effectively created a mechanism for one program to call another program. If both programs continue concurrently, we have created a new job or process to be multiprogrammed. Often, there is a system call specifically for this purpose (create process() or submit job())
			
			- If we create a new job or process, or perhaps even a set of jobs or processes, we should be able to control its execution. This control requires the ability to determine and reset the attributes of a job or process, including the job’s priority, its maximum allowable execution time, and so on (get process attributes() and set process attributes()). We may also want to terminate a job or process that we created (terminate process()) if we find that it is incorrect or is no longer needed.
			
			- The standard C library provides a portion of the system-call interface for many versions of UNIX and Linux. As an example, let’s assume a C program invokes the printf() statement. The C library intercepts this call and invokes the necessary system call (or calls) in the operating system — in this instance, the write() system call. The C library takes the value returned by write() and passes it back to the user program.
			
			- Quite often, two or more processes may share data. To ensure the integrity of the data being shared, operating systems often provide system calls allowing a process to lock shared data. Then, no other process can access the data until the lock is released. Typically, such system calls include acquire lock() and release lock()
			
			- here are so many facets of and variations in process and job control that we next use two examples—one involving a single-tasking system and the other a multitasking system—to clarify these concepts. The MS-DOS operating system is an example of a single-tasking system (it uses a simple method to run a program and does not create a new process), while FreeBSD is an example of a multi-tasking system (the command interpreter may continue running while another program is executed). to start a new process, the shell executes a fork() system call. Then, the selected program is loaded into memory via an exec() system call, and the program is executed.
			
		2) File Management:
			- We first need to be able to create() and delete() files. Either system call requires the name of the file and perhaps some of the file’s attributes. Once the file is created, we need to open() it and to use it. We may also read(), write(), or reposition() (rewind or skip to the end of the file, for example). Finally, we need to close() the file, indicating that we are no longer using it. 
			
			- We may need these same sets of operations for directories if we have a directory structure for organizing files in the file system. In addition, for either files or directories, we need to be able to determine the values of various attributes and perhaps to reset them if necessary. File attributes include the file name, file type, protection codes, accounting information, and so on. At least two system calls, get file attributes() and set file attributes(), are required for this function. Some operating systems provide many more calls, such as calls for file move() and copy()
			
		3) Device Management:
			- A process may need several resources to execute—main memory, disk drives, access to files, and so on. If the resources are available, they can be granted, and control can be returned to the user process. Otherwise, the process will have to wait until sufficient resources are available.
			
			- The various resources controlled by the operating system can be thought of as devices. Some of these devices are physical devices (for example, disk drives), while others can be thought of as abstract or virtual devices (for example, files)
			
			- A system with multiple users may require us to first request() a device, to ensure exclusive use of it. After we are finished with the device, we release() it. These functions are similar to the open() and close() system calls for files.
			
			- Once the device has been requested (and allocated to us), we can read(), write(), and (possibly) reposition() the device, just as we can with files. In fact, the similarity between I/O devices and files is so great that many operating systems, including UNIX, merge the two into a combined file –device structure. In this case, a set of system calls is used on both files and devices. Sometimes, I/O devices are identified by special file names, directory placement, or file attributes.
			
		4) Information Maintenance:
			- Many system calls exist simply for the purpose of transferring information between the user program and the operating system. For example, most systems have a system call to return the current time() and date(). Other system calls may return information about the system, such as the number of current users, the version number of the operating system, the amount of free memory or disk space, and so on
			
			- Another set of system calls is helpful in debugging a program. Many systems provide system calls to dump() memory. This provision is useful for debugging. A program trace lists each system call as it is executed. Even microprocessors provide a CPU mode known as single step, in which a trap is executed by the CPU after every instruction. The trap is usually caught by a debugger.
			
			- In addition, the operating system keeps information about all its processes, and system calls are used to access this information. Generally, calls are also used to reset the process information (get process attributes() and set process attributes())
			
		5) Communication:
			- There are two common models of interprocess communication: the message- passing model and the shared-memory model.
			
			--> In the message-passing model, the communicating processes exchange messages with one another to transfer information. Messages can be exchanged between the processes either directly or indirectly through a common mailbox. Before communication can takeplace, a connection must be opened. The name of the other communicator must be known, be it another process on the same system or a process on another computer connected by a communications network.
			
			- Each computer in a network has a host name by which it is commonly known. A host also has a network identifier, such as an IP address.
			
			- Similarly, each process has a process name, and this name is translated into an identifier by which the operating system can refer to the process. The get hostid() and get processid() system calls do this translation. The identifiers are then passed to the general- purpose open() and close() calls provided by the file system or to specific open connection() and close connection() system calls, depending on the system’s model of communication.
			
			- Most processes that will be receiving connections are special-purpose daemons, which are system programs provided for that purpose. They execute a wait for connection() call and are awakened when a connection is made.The source of the communication, known as the client, and the receiving daemon, known as a server, then exchange messages by using read message() and write message() system calls. The close connection() call terminates the communication.
			
			--> In the shared-memory model, processes use shared memory create() and shared memory attach() system calls to create and gain access to regions of memory owned by other processes. 
			
			- normally, the operating system tries to prevent one process from accessing another process’s memory. Shared memory requires that two or more processes agree to remove this restriction. They can then exchange information by reading and writing data in the shared areas. The form of the data is determined by the processes and is not under the operating system’s control. The processes are also responsible for ensuring that they are not writing to the same location simultaneously.
			
		6) Protection:
			- Protection provides a mechanism for controlling access to the resources provided by a computer system. Historically, protection was a concern only on multiprogrammed computer systems with several users. However, with the advent of networking and the Internet, all computer systems, from servers to mobile handheld devices, must be concerned with protection.
			
			- Typically, system calls providing protection include set permission() and get permission(), which manipulate the permission settings of resources such as files and disks. The allow user() and deny user() system calls specify whether particular users can—or cannot—be allowed access to certain resources	
	

	- hence, a system call is a programmatic way in which a computer program requests a service from the OS.

	- only once the bootstrap program has fully loaded and it has located the OS kernel and loaded it into the main memory do we say that the system is running.
	
	-> FORK system call:
		- used to create a child process.
		
		- a clone of the parent having its own PID, registers, etc.
		
		- returns 0 or 1. 0 indicates child process, 1 means the parent process, -ve => error.
		
		- running fork will keep executing the parent as it is, but creates a new child C1 that executes parallely.
		
		- total 2^n processes if n times fork called. no. of child processes will be 2^n - 1 (removing the inital parent)
		
	-> EXEC system call:
		- replace the current process with a new process.
		
		- PID remains same, but the entire address space and threads are made afresh for the new process.
		
		- used in combination with fork(), to start a new process in the child process.

# PROCESS MANAGEMENT
- a process is a program in execution. each program may be composed of multiple processes

- A system therefore consists of a collection of processes: operating- system processes executing system code and user processes executing user code. Potentially, all these processes can execute concurrently, with the CPU (or CPUs) multiplexed among them. By switching the CPU between processes, the operating system can make the computer more productive

- when a program is first run, it's loaded into memory by assigning it some address space. it may be assigned non-contiguous locations in the physical address space. for this reason, there is a virtual memory image that contains a these addresses of the process in a contiguous fashion (compiler allocates the virtaul address space).

- the physical address space reserves a part for OS calls and programs. these are also mapped in the virtual memory space of every process, as every process may require the usage of OS files/resources that are not in the address space assigned to that process.

- the address space of a process consists of the kernel space and the user space. the kernel space consists of the kernel stack (for keeping track of system calls), and the state of the process in the OS. the user space consists of heap, stack, data, code.

- contents of a process: A process is more than the program code, which is sometimes known as the text section. It also includes the current activity, as represented by the value of the program counter and the contents of the processor’s registers. A process generally also includes the 
	1) process stack, which contains temporary data (such as function parameters, return addresses, and local variables)
	2) data section, which contains global variables
	3) heap, which is memory that is dynamically allocated during process run time.

	-> Dual mode of operation of a process:
		- a process operates either in priviledged or non-priviledged mode at a time. in non-priviledged, the process code is run but in priviledged mode, the OS (kernel) runs the instructions as only it can run those and provide those resources. (like for hardware access, I/O, system calls, etc.). so, the Virtual Map (VM) of every process has a fixed mapping to the OS code in the physical space.
		
	-> Interrupts:
		- interrupts a running process.
		- interrupt forces the CPU to start executing instructions from a specific address. the instructions that are executed on the occurrence of an interrupt are called interrupt handler or subroutines.
		- interrupt vector contains addresses of all interrupt handlers (like a predefined table for specific interrupts). so the CPU sees which interrupt handler has occurred from this vector.
		(a) Physical event: eg: keyboard control, mouse control, etc.
		(b) occurrence of errors/exceptions: eg: buffer overflow, division by 0. handling errors in this case by interrupts are called TRAPS.
		(c) software generated interrupts: eg: for system calls
		- OS is interrupt driven software. (since system calls are S/W generated interrupts, and they are the interface between the user apps and the OS)

- a thread is the unit of execution within a process. a process can have anywhere from one to multiple threads.

	-> Process state:
			
											NEW -> READY <-> RUNNING -> TERMINATED
													  /\   	   ||
													  || 	   \/
													   WAIT/BLOCK		
	
		- state of a process is defined in part by the current activity of the process (what is the process currently doing?) the different states that a process can be in are:
			1) New: the process is newly being created.
			2) Ready: once the process has been created, it has to be assigned to a processor to be executed. this can only be done once the process is in the "ready" state. LONG TERM SCHEDULER brings processes from the "New" state to the "ready" queue. increase the number of processes in ready queue to increase multiprogramming.
			3) Running: once the process is in the ready state, the Scheduler dispatches the process to the processor so that it can begin execution. Instructions of the process are being executed. one process per CPU/Core. Process brought from Ready to Running state by SHORT TERM SCHEDULER.
			4) Waiting/Blocked: Process is waiting for some event to occur (like the completion of an I/O operation or reception of a signal). After the event, the process goes back to the ready state, not directly the running state.
			5) Terminated: The process has completed its execution. process is deallocated from the memory (RAM)
			6) Suspend-wait state: if the RAM is getting filled due to filled Wait queues, the processes can be sent to the suspended queue in the secondary memory by the MEDIUM TERM SCHEDULER. Backing store: if a suspended-wait process gets over or is ready to become ready, it can go directly to the suspend-ready state if the ready queue is filled
			7) Suspend-ready state: same as above but for the ready queue getting filled
	
- PCB (Process Control Block) - used to represent each process in the OS. contains information about the process.
contains info such as: 
	1) process ID - unique for every process
	2) process state
	3) program counter - contains address of the next instruction to be exectued. 
	4) CPU registers - tells the registers used by a particular process.
	5) CPU Scheduling information - priority of the processes, pointer to the scheduling queue. used to determine which process to execute next.
	6) Memory management - memory being used by a process.
	7) Accounting - keeps account of resources used by a process (CPU/time/etc).
	8) I/O Status - I/O devices being used by a process stored in this.


#THREADS
- a thread is a basic unit of CPU utilization.

- a thread comprises of a thread ID, a program counter, a register set, and a stack. it shares with other threads of the same process its code section, data section, other OS resources like open files and signals. each thread has its own stack and registers. a new child process will however have everything replicated again.

- SHARED: Code, data, files
  NEW: registers, stack

- a single core of a processor can handle only one thread at a time. multicore system means multithreading can be done (each core can run a single thread, so we can run multiple threads of a process at the same time to get the process done faster). hyperthreading when each single core can be split into a couple of logical cores, allowing more than one thread to run on each physical core.

- alternative solution is to create a new process to service each task. but process creation is time consuming and resource intensive, however. If the new process will perform the same tasks as the existing process, why incur all that overhead? It is generally more efficient to use one process that contains multiple threads.

- Multithreading an interactive application may allow a program to continue running even if part of it is blocked or is performing a lengthy operation, thereby increasing responsiveness to the user. This quality is especially useful in designing user interfaces.

	-> PROCESSES vs THREADS:
		- process requires an api call (fork()) but thread does not.
		- processes are treated independently, threads are treated as a single process.
		- processes have slower context switching as compared to threads (this is because context switching in processes involves saving the state to PCB, which involves the invervention of the OS, increasing overhead, unlike in threads).
		- processes can be blocked independently without affecting other, blocking a thread will block the entire process.
		- the child process can run independent of the parent process, but if we block a thread, the kernel will block all the other threads of that process as well (since threads are user-level, not kernel and so it does not distinguish between the threads of a process). this is assuming a unicore, uniprocessor system as only one thread can run per core and no hyperthreading is involved.
		
		- Even in systems with multiple cores, each core typically handles one thread or process at a time. However, the appearance of simultaneous execution of multiple processes or threads is achieved through rapid context switching. The operating system's scheduler allocates CPU time to different threads or processes, giving the illusion of concurrent execution.

		- The concept of parallelism arises when multiple cores are available, allowing different cores to handle different threads or processes simultaneously. In this case, true parallelism is achieved, and each core can independently execute instructions for a separate thread or process.


- multiple threads => multiple tasks at a time.
- benefits include:
	1) Responsiveness: let lengthy operation perform on the side (on another thread).
	2) Resource sharing: threads share resource and memory, allows an application to have different threads of activity within the same address space.
	3) Economy: alternative to multithreading would be multiple processes for each task, but that is costly as each process would require it's own memory, resources, etc.
	4) Utilizes multiprocessor architecture: Note - a single-threaded process can only run on one CPU, even if multiple processors are available. multithreaded process means that process can split up tasks into multiple threads which can be split among the processors.
	
- two types of threads: user threads and kernel threads.
	- user level are managed by the user level library (like Pthreads (POSIX)), but the kernel level threads are managed by the OS => requires system calls.
	- user level threads are generally faster as they are at a user level and do not require system calls, hence reducing the overhead.
	- user level threads have faster context switching, and are independent of the other, unlike kernel level threads.
	- if a user level thread performs some blocking operation (like I/O), it will cause the entire process to be blocked, but not in the case of kernel-level threads.
	- both kernel level and user level threads share data (cuz they are still ultimately threads only)
	
		note: context switch time : Process > Kernel-level Threads > User-level Threads

	-> Hyperthreading (aka Simultaneous multithreading):
		- hyperthreaded systems allow their processor cores' resources to become multiple logical processors for performance, and so each thread can now execute on each of these logical processors. if n(logicalProcessors) > n(cores), then hyperthreading is enabled on the system. this enables a single physical core to execute two threads at the same time.
		
	-> Multicore Programming:
		- place multiple computing cores on a single chip. Each core appears as a separate processor to the operating system
		- Whether the cores appear across CPU chips or within CPU chips, we call these systems multicore or multiprocessor systems. Multithreaded programming provides a mechanism for more efficient use of these multiple computing cores and improved concurrency.
		- example: Consider an application with four threads. On a system with a single computing core, concurrency merely means that the execution of the threads will be interleaved over time (one thread after the other on a single core), because the processing core is capable of executing only one thread at a time. On a system with multiple cores, however, concurrency means that the threads can run in parallel, because the system can assign a separate thread to each core
		NOTE: although only n number of threads can run parallely on an n-core system (without hyperthreading), there can still exist more than n threads at any given moment on the system, but only n of these will be "active" concurrently.
		- there are two types of parallelism: data parallelism and task parallelism.
		1) Data Parallelism:
			- Data parallelism focuses on distributing subsets of the same data across multiple computing cores and performing the same operation on each core.
			- Consider, for example, summing the contents of an array of size N. On a single-core system, one thread would simply sum the elements [0] . . . [N − 1]. On a dual-core system, however, thread A, running on core 0, could sum the elements [0] . . . [N/2 − 1] while thread B, running on core 1, could sum the elements [N/2] . . . [N − 1].
		2) Task Parallelism:
			- Task parallelism involves distributing not data but tasks (threads) across multiple computing cores. Each thread is performing a unique operation.
			- Different threads may be operating on the same data, or they may be operating on different data
			- example: performing a different mathematical operation on the same array in two separate threads.
			
	-> Multithreading Models:
		- support for threads may be provided either at the user level, for user threads, or by the kernel, for kernel threads. User threads are supported above the kernel and are managed without kernel support, whereas kernel threads are supported and managed directly by the operating system.
		1) Many-to-one model:
			- maps many user-level threads to one kernel thread. Thread management is done by the thread library in user space, so it is efficient.
			- the entire process will block if a thread makes a blocking system call. Also, because only one thread can access the kernel at a time, multiple threads are unable to run in parallel on multicore systems
			- very few systems continue to use the model because of its inability to take advantage of multiple processing cores.
		
		2) One-to-one model:
			- The one-to-one model (Figure 4.6) maps each user thread to a kernel thread. It provides more concurrency than the many-to-one model by allowing another thread to run when a thread makes a blocking system call.
			- allows multiple threads to run in parallel on multiprocessors. The only drawback to this model is that creating a user thread requires creating the corresponding kernel thread. Because the overhead of creating kernel threads can burden the performance of an application, most implementations of this model restrict the number of threads supported by the system.
			- Linux, along with the family of Windows operating systems, implement the one-to-one model.
		
		3) Many-to-many model:
			- multiplexes many user-level threads to a smaller or equal number of kernel threads.
			- The many-to-many model suffers from neither of these shortcomings of one-to-one or many-to-one: developers can create as many user threads as necessary, and the corresponding kernel threads can run in parallel on a multiprocessor. Also, when a thread performs a blocking system call, the kernel can schedule another thread for execution
			- One variation on the many-to-many model still multiplexes many user- level threads to a smaller or equal number of kernel threads but also allows a user-level thread to be bound to a kernel thread. This variation is sometimes referred to as the two-level model
		
	-> Difference between fork() and exec() system calls:
The fork() system call is used to create an exact copy of a running process and the created copy is the child process and the running process is the parent process. Whereas, exec() system call is used to replace a process image with a new process image. Hence there is no concept of parent and child processes in exec() system call.

In fork() system call the parent and child processes are executed at the same time. But in exec() system call, if the replacement of process image is successful, the control does not return to where the exec function was called rather it will execute the new process. The control will only be transferred back if there is any error.

- so exec just replaces the currently running process with the contents of the mentioned file in the parameter, hence the pid of the exec file remains same if it is called from the parent process.  The current process is just turned into a new process and hence the process id PID is not changed, this is because we are not creating a new process we are just replacing a process with another process in exec.

- If the currently running process contains more than one thread then all the threads will be terminated and the new process image will be loaded and then executed. PID of the process is not changed but the data, code, stack, heap, etc. of the process are changed and are replaced with those of newly loaded process.

	-> Thread Libraries:
		- A thread library provides the programmer with an API for creating and managing threads.
		- There are two primary ways of implementing a thread library. 
		
		- The first approach is to provide a library entirely in user space with no kernel support. All code and data structures for the library exist in user space. This means that invoking a function in the library results in a local function call in user space and not a system call.
		- The second approach is to implement a kernel-level library supported directly by the operating system. In this case, code and data structures for the library exist in kernel space. Invoking a function in the API for the library typically results in a system call to the kernel.
		
		- Three main thread libraries are in use today: POSIX Pthreads, Windows, and Java. Pthreads, the threads extension of the POSIX standard, may be provided as either a user-level or a kernel-level library.
		- For POSIX and Windows threading, any data declared globally—that is, declared outside of any function—are shared among all threads belonging to the same process. Because Java has no notion of global data, access to shared data must be explicitly arranged between threads. Data declared local to a function are typically stored on the stack. Since each thread has its own stack, each thread has its own copy of local data.
		- two general strategies for creating multiple threads: asynchronous threading and synchronous threading.
		
		1) Asynchronous threading:
			- With asynchronous threading, once the parent creates a child thread, the parent resumes its execution, so that the parent and child execute concurrently.
			- Each thread runs independently of every other thread, and the parent thread need not know when its child terminates. Because the threads are independent, there is typically little data sharing between threads.
			
		2) Synchronous threading:
			- the parent thread creates one or more children and then must wait for all of its children to terminate before it resumes —the so-called fork-join strategy
			- the threads created by the parent perform work concurrently, but the parent cannot continue until this work has been completed. Once each thread has finished its work, it terminates and joins with its parent. 
			- Typically, synchronous threading involves significant data sharing among threads. For example, the parent thread may combine the results calculated by its various children.
			
	-> Implicit Threading:
		- One way to address difficulties related to threading and better support the design of multithreaded applications is to transfer the creation and management of threading from application developers to compilers and run-time libraries.
		
		-> Thread pools:
			- The first issue concerns the amount of time required to create the thread, together with the fact that the thread will be discarded once it has completed its work
			- Another issue is that If we allow all concurrent requests to be serviced in a new thread, we have not placed a bound on the number of threads concurrently active in the system. Unlimited threads could exhaust system resources, such as CPU time or memory.
			- The general idea behind a thread pool is to create a number of threads at process startup and place them into a pool, where they sit and wait for work. When a server receives a request, it awakens a thread from this pool—if one is available —and passes it the request for service. Once the thread completes its service, it returns to the pool and awaits more work. If the pool contains no available thread, the server waits until one becomes free.
			- Thread pools offer these benefits:
				- 1. Servicing a request with an existing thread is faster than waiting to create
				a thread.
				- 2. A thread pool limits the number of threads that exist at any one point.
				This is particularly important on systems that cannot support a large
				number of concurrent threads.
				- 3. Separating the task to be performed from the mechanics of creating the
				task allows us to use different strategies for running the task. For example,
				the task could be scheduled to execute after a time delay or to execute
				periodically.

	->THREAD ISSUES:

	- if a process is running multiple threads, and one of the thread calls a fork() system call, then does only that thread get duplicated, or all the threads of that process? for this, UNIX systems have two types of fork() calls to serve the two different scenarios.

	if exec is called immediately after forking, only the specific thread should be duplicated as all other threads will be replaced by exec. if exec is not called after forking, all threads should be duplicated.

	- running a exec call, will, however, replace all the threads of that process with the new process contents.

	- thread cancellation:
		- terminating a thread before it has completed.
		- a thread that is to be cancelled is referred to as the target thread.
	
	
	
# CPU SCHEDULING:
- the process scheduler selects an available process (possibly from a set of several available processes) for program execution on the CPU. For a single-processor system, there will never be more than one running process. If there are more processes, the rest will have to wait until the CPU is free and can be rescheduled.

	-> Scheduling Queues:
		- As processes enter the system, they are put into a JOB QUEUE, which consists of all processes in the system.
		
		- The processes that are residing in main memory and are ready and waiting to execute are kept on a list called the READY QUEUE. This queue is generally stored as a linked list. A ready-queue header contains pointers to the first and final PCBs in the list. Each PCB includes a pointer field that points to the next PCB in the ready queue.
		
		- Suppose the process makes an I/O request to a shared device, such as a disk. Since there are many processes in the system, the disk may be busy with the I/O request of some other process. The process therefore may have to wait for the disk. The list of processes waiting for a particular I/O device is called a DEVICE QUEUE. Each device has its own device queue
		
		- A new process is initially put in the ready queue. It waits there until it is selected for execution, or dispatched.
		
		- Once the process is allocated the CPU and is executing, one of several events could occur:
			• The process could issue an I/O request and then be placed in an I/O queue.
			• The process could create a new child process and wait for the child’s termination.
			• The process could be removed forcibly from the CPU, as a result of an interrupt, and be put back in the ready queue.
		  In the first two cases, the process eventually (after wait operation is completed) switches from the waiting state to the ready state and is then put back in the ready queue. A process continues this cycle until it terminates, at which time it is removed from all queues and has its PCB and resources deallocated.
		  
	-> Schedulers:
		- The operating system must select, for scheduling purposes, processes from these queues in some fashion. The selection process is carried out by the appropriate scheduler.
		
		- Often, in a batch system, more processes are submitted than can be executed immediately. These processes are spooled to a mass-storage device (typically a disk), where they are kept for later execution. The LONG-TERM SCHEDULER, or JOB SCHEDULER, selects processes from this pool and loads them into memory for execution.
		
		- Thus it plans the CPU scheduling for batch jobs. Processes, which are resource intensive and have a low priority are called batch jobs. These jobs are executed in a group or bunch.
		
		- The SHORT-TERM SCHEDULER, or CPU SCHEDULER, selects from among the processes that are ready to execute and allocates the CPU to one of them.
		
		- The primary distinction between these two schedulers lies in frequency of execution. 
		
		- The short-term scheduler must select a new process for the CPU frequently. A process may execute for only a few milliseconds before waiting for an I/O request. Often, the short-term scheduler executes at least once every 100 milliseconds. Because of the short time between executions, the short-term scheduler must be fast.
		
		- The long-term scheduler executes much less frequently; minutes may separate the creation of one new process and the next. The long-term scheduler controls the degree of multiprogramming (the number of processes in mem- ory). If the degree of multiprogramming is stable, then the average rate of process creation must be equal to the average departure rate of processes leaving the system. Thus, the long-term scheduler may need to be invoked only when a process leaves the system. Because of the longer interval between executions, the long-term scheduler can afford to take more time to decide which process should be selected for execution.
		
		- An I/O-bound process is one that spends more of its time doing I/O than it spends doing computations. A CPU-bound process, in contrast, generates I/O requests infrequently, using more of its time doing computations.
		
		- if all processes are I/O bound, the ready queue will almost always be empty, and the short-term scheduler will have little to do. If all processes are CPU bound, the I/O waiting queue will almost always be empty, devices will go unused, and again the system will be unbalanced. hence, the long-term scheduler needs to select a good process mix of I/O bound and CPU bound processes.
		
		- Some operating systems, such as time-sharing systems, may introduce an additional, intermediate level of scheduling, called the MEDIUM-TERM SCHEDULER
		
		- The key idea behind a medium-term scheduler is that sometimes it can be advantageous to remove a process from memory (and from active contention for the CPU) and thus reduce the degree of multiprogramming. Later, the process can be reintroduced into memory, and its execution can be continued where it left off. This scheme is called swapping. The process is swapped out, and is later swapped in, by the medium-term scheduler.
		
		-  A running process may be suspended because of an I/O request or by a system call. Such a suspended process is then removed from the main memory and is stored in a swapped queue in the secondary memory in order to create a space for some other process in the main memory. This is done because there is a limit on the number of active processes that can reside in the main memory.The medium-term scheduler is in charge of handling the swapped-out process. 
		
		- once the suspending condition is removed, the medium terms scheduler attempts to allocate the required amount of main memory and swap the process in & make it ready. Thus, the medium-term scheduler plans the CPU scheduling for processes that have been waiting for the completion of another process or an I/O task.
		
		- all of these schedulers work to put a process into execution by the CPU. they just serve different kinds of processes.
	
	-> Context Switching:
		- When an interrupt occurs, the system needs to save the current context of the process running on the CPU so that it can restore that context when its processing is put on hold, essentially suspending the process and then resuming it. The context is represented in the PCB of the process. It includes the value of the CPU registers, the process state (see Figure 3.2), and memory-management information. Generically, we perform a state save of the current state of the CPU, be it in kernel or user mode, and then a state restore to resume operations.
		- Switching the CPU to another process requires performing a state save of the current process and a state restore of a different process. This task is known as a context switch.
		- When a context switch occurs, the kernel saves the context of the old process in its PCB and loads the saved context of the new process scheduled to run. Context-switch time is pure overhead, because the system does no useful work while switching.
	
	

-> OPERATIONS ON PROCESSES:
1) Process Creation:
		- During the course of execution, a process may create several new processes. As mentioned earlier, the creating process is called a parent process, and the new processes are called the children of that process. Each of these new processes may in turn create other processes, forming a tree of processes.
		
		- identify processes according to a unique process identifier (or pid), which is typically an integer number. The pid provides a unique value for each process in the system, and it can be used as an index to access various attributes of a process within the kernel.
		
		- The init process (which always has a pid of 1) serves as the root parent process for all user processes. Once the system has booted, the init process can also create various user processes
		
		- In general, when a process creates a child process, that child process will need certain resources (CPU time, memory, files, I/O devices) to accomplish its task.
		
		- The parent may have to partition its resources among its children, or it may be able to share some resources (such as memory or files) among several of its children. Restricting a child process to a subset of the parent’s resources prevents any process from overloading the system by creating too many child processes.
		
		- A new process is created by the fork() system call. The new process consists of a copy of the address space of the original process.
		
		- Both processes (the parent and the child) continue execution at the instruction after the fork(), with one difference: the return code for the fork() is zero for the new (child) process, whereas the (nonzero) process identifier of the child is returned to the parent.
		
		- After a fork() system call, one of the two processes typically uses the exec() system call to replace the process’s memory space with a new program. so if the exec call is made under the child process, the child process is entirely replaced by the process of the new program, but the original parent process of the child remains the same and keeps executing. exec() overlays the process’s address space with a new program
		
		- An address space is a range of valid addresses in memory that are available for a program or process. That is, it is the memory that a program or process can access.
		
		- The size of an address space can be made larger than that of physical memory by using a memory management technique called virtual memory. A virtual memory, also known as a page file, is actually a physical file on disk that acts like an additional RAM or RAM module. Thus, an address space consists of both physical memory and virtual memory.
		
		- the wait(NULL) system call is used when we want a parent process to wait for the termination of its child processes before it can execute.
		
		- Because the child is a copy of the parent, each process has its own copy of any data.
	
2) Process Termination: 
	- process terminates when it finishes executing its final statement and asks the OS to delete it by using the exit() system call.
	
	- at that point, the process may return a status value to its parent process (via the wait(&status) system call). this is so that the parent knows that the child process has completed its execution and it is terminating.
	
	- all resources of the process are deallocated
	
	- the parent of a process can also terminate its child processes. a parent may do so cuz of:
		i) the child has exceeded its usage of the allocated resources.
		ii) the task assigned to the child is not required by the parent or the process tree.
		iii) the parent is exiting (terminating), and some OS's do not allow the children to execute if the parent is terminated. (aka CASCADING TERMINATION)

3) Interprocess Communication: (IPC)
	- involves communication of one process with another.
	
	- process executing concurrently may either be independent or cooperating processes.
	
	- independent: cannot affect/be affected by other processes in the system.
	  cooperating: can affect/be affected by other processes in the system. eg: any process that shares data with other process . so, interprocess communication will be required in cooperating processes
	  
	- reasons for process cooperation:
		i) information sharing (like shared files/memory)
		ii) computational speedup (by breaking up a process into several subprocesses that need to communicate with each other)
		iii) modularity (dividing the system into multiple modules that need to communicate with each other)
		iv) convenience (multiple processes running at the same time need to cooperate with each other without clashing)
	
	- two fundamental models of IPC: shared memory and message passing
	
	- Message passing is useful for exchanging smaller amounts of data, because no conflicts need be avoided. Message passing is also easier to implement in a distributed system than shared memory. 
	
	- Shared memory can be faster than message passing, since message-passing systems are typically implemented using system calls and thus require the more time-consuming task of kernel intervention. In shared-memory systems, system calls are required only to establish shared-memory regions. Once shared memory is established, all accesses are treated as routine memory accesses, and no assistance from the kernel is required.
	
	- systems with several processing cores indicates that message passing provides better performance than shared memory on such systems. Shared memory suffers from cache coherency issues, which arise because shared data migrate among the several caches.
	
		a) Shared memory:
			- a region of memory that is shared by cooperating processes is established.
			- processes can then exchange information by reading/writing data to this shared region.
			- Address space is set of logical addresses that a process references in its code. 
			- the shared memory region resides in the address space of the process that initiates the communication.
			- other processes that wish to communicate using this shared memory segment must attach it to their address space.
			- normally, one process cannot access another process' memory. shared memory requires that two or more processes remove this restriction.
			
		--> PRODUCER CONSUMER PROBLEM:
			- a producer process produces info. that is consumed by a consumer process. (eg: a compiler may produce assembly code, which is consumed by an assembler. or, client-server model where the server produces web pages and the client consumes them.)
			- the problem is that the consumer should consume only what the producer has produced, and not anything that the producer has not produced.
			- this can be solved using shared memory. the producer and consumer can run concurrently by making available a buffer of items that can be filled by the producer and emptied by the consumer. so, the producer can produce one item while the consumer is consuming another item. this process must be synchronized so that the consumer can't consume an item that has not yet been produced.
			- two kinds of buffers: unbounded buffer, where the producer can always produce new items (no practical limit on the size of the buffer). bounded buffer, where the buffer size is fixed. the producer may have to wait before producing new items when the buffer is full, and the consumer may have to wait to consume items when the buffer is empty and nothing is produced. 
				
		b) Message passing:
			- communication takes place by means of messages exchanged between the cooperating processes.
			- the message is sent to the kernel, which then relays it to the receipient process
			- particularly useful in a distributed environment where the communicating processes may reside on different computers connected by a network (like chatting across the internet).
			- allows send(message) and receive(message).
			- suppose P and Q want to communicate with each other. they must be able to send/receive messages from each other. for this, a COMMUNICATION LINK must exist between them. this link can be logically implemented by:
			1) direct/indirect communication:
				- processes that want to communicate must have a way to refer to each other. the "NAMING" feature is used for this.
				-> DIRECT COMMUNICATION, each process that wants to comm. must explicitly name the receipient/sender of the message. eg: send(P, msg) //msg sent to P.
					receive(Q, msg) //receive msg from Q.
					- a link is established automatically between every pair of communicating processes. the processes need to know only each others identity to communicate. a link exists between exactly two processes, and between each pair of processes, there exists exactly one link.
					- Symmetry in addressing, where both the sender/receiver processes must name the other in order to communicate.			
					- a variant of direct comm: only sender names the recipient, recippient is not required to name the sender. eg: send(P, msg), receive(id, msg). it can receive messages from any process as id can vary (can be the id of any process). this scheme employs Asymmetry in addressing.
					- disadvantage of both schemes: limited modularity. if the name of the process is changed, we would have to change the name used in all process definitions and communications.
					
				-> INDIRECT COMMUNICATION, messages are sent to and received from mailboxes, or ports.
					- a mailbox is abstractly an object into which messages can be placed by processes and from which messages can be removed. each mailbox is uniquely identifiable.
					- two processes can communicate only if the processes have a shared mailbox.
					eg: send(A,msg), receive(A,msg) : send msg to mailbox A, receive msg from mailbox A.
					- a link is created between a pair of processes only if they have a shared mailbox.
					- a link can be associated with more than two processes.
					- between each pair of communicating processes, there may be a number of different ilnks, with each link corresponding to one mailbox.
					- in case of clashes where more than two processes share the mailbox, we can:
						1) allow a link to be associated with atmost two processes.
						2) allow at most one process at a time to execute a receive() operation.
						3) allow the system to arbitrarily choose which process (not both) receives the message.
					- a mailbox may be owned either by a process or the OS.
			2) SYNCHRONIZATION, message passing may be either blocking or non-blocking (synch. or asynch.)
				- synchronous send: the process that sends the message
				is blocked until the message is received by a process or the mailbox.
				- asynchronous send: the sending process sends the message and resumes operation.
				- synchronous receive: the receiver is blocked until a message is available.
				- asynchronous receive: the receiver is not blocked until it receives a message.
				
			3) SOCKETS, used for communication in a client-server based system.
				- a socket is an endpoint for communication.
				- one socket for each process communicating over a network.
				- a socket is identified by an IP address concatenated with a PORT number.
				- the server waits for incoming client requests by listening to a specified port. once a request is received, the server accepts a connection from the client socket to complete the connection.
				- servers implementing specific services listen to well-known ports. all ports below 1024 are considered well-known, used for implementing standard services. (like http, telnet, web, etc.)




# PROCESS SCHEDULING:
- in a multiprogrammed system, a program should always keep running in the CPU (the CPU should not be kept idle). when a process goes for I/O, another process should be loaded in depending on what scheduling algorithm is used.

- process execution consists of a cycle of CPU execution and I/O wait. processes alternate between these two states.

- CPU burst: the time that the process is under CPU exec.
  I/O burst: the time that the process is under I/O opn.
  
- just before the termination of a process, there will always be one final CPU burst.

- when the CPU becomes idle, the OS must select one of the processes in the ready queue to be executed. The selection process is carried out by the short term scheduler (or CPU scheduler). The scheduler selects a process from the processes in memory that are ready to execute and allocates the CPU to that process.

- the dispatcher is the module that gives the CPU control to the process that is selected by the short-term scheduler. time taken by the dispatcher to stop one process and start another is called DISPATCH LATENCY. ideally, the dispatch latency should be less. 


-> PREEMPTIVE AND NON PREEMPTIVE SCHEDULING:
- CPU scheduling decisions can take place under the following four scenarios:
	1) process switches from the running state to the waiting state (like for some I/O operation).
	2) process switches from the running state to the ready state (when an interrupt occurs, halting the execution, like preemption because of time quantum, priority, etc).
	3) process switches from the waiting state to the ready state (after completion of I/O operation) 
	4) when a process terminates.
	
	- for 1 and 4, a new process should be chosen by the CPU. when scheduling takes place under these circumstances, the scheduling scheme is called non-preemptive or cooperative. so when a process is in the running state, it cannot be withdrawn under non-preemptive scheduling.
	- for 2 and 3, there is a choice whether the current process that has not finished execution should be executed, or some other process. when scheduling takes place under these circumstances, the scheduling scheme is called preemptive. a running process can be withdrawn for another process.
	
	
	PREEMPTIVE: SRTF, LRTF, ROUND ROBIN, PRIORITY BASED
	NON-PREEMPTIVE: SFJ, LJF, FCFS, HRRN (Highest Response Ratio Next)
	
	
	
# PROCESS SYNCHRONIZATION:
- cooperating messages can directly affect/be affected by other processes. they either share a logical address space (shared memory system) or store data only through files or messages. (memory passing system)

- concurrent access to shared data may lead to data inconsistency, which arises the need for synchronization. (like in producer consumer problem in shared memory system)

- a situation in which several processes access and manipulate the same data concurrently and the outcome of the execution depends on the particular order in which the access takes place, is called RACE CONDITION. (eg: producer and consumer trying to produce and consume an item at the same time respectively).


	-> PRODUCER-CONSUMER PROBLEM:
		- A producer process produces information that is consumed by a consumer process. For example, a compiler may produce assembly code that is consumed by an assembler.

		- server as a producer and a client as a consumer. For example, a web server produces (that is, provides) HTML files and images, which are consumed (that is, read) by the client web browser requesting the resource.

		- One solution to the producer–consumer problem uses shared memory. To allow producer and consumer processes to run concurrently, we must have available a BUFFER OF ITEMS that can be filled by the producer and emptied by the consumer.

		- This buffer will reside in a region of memory that is shared by the producer and consumer processes. A producer can produce one item while the consumer is consuming another item. The producer and consumer must be synchronized, so that the consumer does not try to consume an item that has not yet been produced.

		- Two types of buffers can be used. The unbounded buffer places no practical limit on the size of the buffer. The consumer may have to wait for new items, but the producer can always produce new items. The bounded buffer assumes a fixed buffer size. In this case, the consumer must wait if the buffer is empty, and the producer must wait if the buffer is full.

		for a producer process:
		- The shared buffer is implemented as a circular array with two logical pointers: in and out. The variable in points to the next free position in the buffer; out points to the first full position in the buffer. The buffer is empty when in == out; the buffer is full when ((in + 1) % BUFFER SIZE) == out.

		int count = 0;
		void producer(void){
			int itemP;
			while (1){
				Produce_item(itemP);
				while (count == n); //if buffer is empty, cannot produce more items. have to wait till count decremented (that is, when the consumer consumes something)
				buffer[in] = itemP;
				in = (in+1) % BUFFER_SIZE;
				count = count+1;
			}
		}

		for a consumer process:
		- The consumer process has a local variable next consumed in which the item to be consumed is stored.
		as long as the queue is empty (in == out), nothing should be consumed. when the producer produces something and in is not = out, next_consumed item = buffer[out]. then, the item is consumed, and out is incremented to update the position of the first filled item post-consumption.	

		void producer(void){
			int itemC;
			while (1){
				while (count == 0); //if buffer is full, cannot consume more items. have to wait till count incremented (that is, when the producer produces something)
				itemC = buffer[out];
				out = (out+1) % BUFFER_SIZE;
				count = count-1;
			}
		}

		here, count = count + 1 has the following steps in the CPU:
		1) load Rp, m[count]
		2) increment Rp
		3) store m[count], Rp

		- suppose initially there are 3 items in the buffer. while storing a document, context switch occurs after step 2 and before step 3. so count has not been updated, but Rp has been (has the value 4). now, the consumer consumes an item, and the consumer also context switches at step 2 of its count -= 1 operation. Rc would have the value 2. but count is still 3. now, the store m[count], Rp gets executed. hence, count becomes = 4. then, step 3 of consumer gets executed, and so count = 2, whereas there are 3 items in the buffer. hence, sync problem.


	-> PRINTER SPOOLER PROBLEM:
		- basically if there are multiple users accessing a printer, a spooler directory is maintained. all the docs are added to this SD, and one-by-one are accessed by the printer. the code for adding files to SD is VVV
		1) load Ri, m[in]
		2) store SD[Ri], "filename"
		3) INCR Ri
		4) store m[in], Ri

		- suppose already 3 items in SD (at 0,1,2). now, in = 3. 1) => R1 = 3; 2) => SD[3] = "file3"; 3) => R1 = 4;
		now, context switch occurs.
			1) => R2 = 4; 2) => SD[3] = "file4"; 3) => R2 = 4; 4) => in = 4; again context switch to P1, in = 4.
		- but the problem is that file3 is lost. file4 was written in its position.


	-> CRITICAL SECTION PROBLEM:
		- each process has a segment of code called critical section. in this, the process may be changing common vars, updating table, etc. (accessing shared memory).
		
		- critical section contains that code that deals with common data between processes. access to this section/data should hence be synchronized (like the common buffer and Count var in producer-consumer problem)
		
		- thee critical section problem is used to design a set of protocols which can ensure that the Race condition among the processes will never arise.
		
		- In order to synchronize the cooperative processes, our main task is to solve the critical section problem. We need to provide a solution in such a way that the following conditions can be satisfied.
		
		- no two processes are executing in their critical sections at the same time => when one proces is manipulating shared data, no other process is allowed to do so.
		
		1) each process must request permission to enter its CRITICAL SECTION
		2) The section of code implementing the request is the ENTRY SECTION
		3) the critical section may be followed by an EXIT SECTION
		4) remaining code is the REMAINDER SECTION.
		
		do{
			entry section;
				critical section;
			exit section;
				remainder section;
		} while (TRUE);
		
		a solution to the critical section problem must satisfy the following three:
		1) Mutual exclusion: no two processes can be in their critical sections.
		2) Progress: if no process is executing in its critical section and now multiple processes want to, only those processes that are not in their remainder sections can take decision as to which process will go in their critical section (without delay)
		3) Bounded waiting: there exists a limit on the number of processes allowed to enter their critical sections after a process has made a request to enter it and before the request is granted.
		4) No assumption related to H/W or speed of the system.
		
	
	-> Peterson's Solution:
		- a classic software-based solution to the critical sectino problem.
		
		- good algorithmic description of solving the critical section problem at a software level.
		
		- restricted to two processes that alternate execution bw their critical and remainder sections. let processes be Pi and Pj
		
		- two data items must be shared between the processes:
			1) int turn : indicates whose turn it is to enter its critical section (i or j)
			2) boolean flag[2]: indicates whether a process is ready to enter its critical section.
			
			structure of process Pi in Peterson's solution:
			do{
				flag[i] = true;
				turn = j;
				while (flag[j] && turn == j);
					critical section;
				flag[i] = false;
					remainder section;
			} while (TRUE);
			
			structure of process Pj in Peterson's solution:
			do{
				flag[j] = true;
				turn = i;
				while (flag[i] && turn == i);
					critical section;
				flag[j] = false;
					remainder section;
			} while (TRUE);
			
			
			what this ensures is that as long as it's j's turn, the while loop keeps running and i is not able to enter its critical section. similarly for Pj.
			
			turn is a shared variable, so only one process can run in the critical section at a time. so, MUTUAL EXCLUSION is ensured in this solution. satisfies PROGRESS as well. satisfies BOUNDED WAITING as well. but limited to only two processes.	
			
	-> Solution using Locks:
		do {
			acquire lock;
				CS;
			release lock;
		}
		
		1) while (lock==1);
		2) lock=1;
		3) ------ CS
		4) lock=0;
		
		- this can work for many processes but does not ensure mutual exclusion. if P1 is preempted after line 1 (assuming lock is 0 initially), then P2 will also skip 1) as lock == 0. now, both P1 and P2 can clear entry section and enter the critical section.
			
			
	-> TEST AND SET LOCK:
		- a hardware based solution to the critical section problem.
		- shared lock variable, which takes two values, 0 or 1. (0=>unlocked, 1=>locked)
		- before entering crit.sec., process enquires about the lock. this is the testing part.
		- if locked, process keeps waiting till lock is unlocked (basically some other process is in its critical section right now)
		- if not locked, it takes the lock, sets the lock, executes the critical section.
		
		//target is the shared lock variable. it is a pointer as it is shared between all processes.
		boolean TestAndSet( boolean *target ){
			boolean rv = *target; //if target is true, some process is already executing. so current process enters while loop. if it is false, rv is false, while loop breaks, current process goes in critical section. but the *target = TRUE in the next line ensures that any other process gets stuck in the while loop as for them, the TestAndSet function will return TRUE (the current process set the LOCK TO TRUE)
			*target = TRUE;
			return rv;
		}
		- this is an atomic operation. when a process checks/sets a lock, all this happens as a single operation that is uninterrupted by other processes. hence, no preemption can occur in between unlike in the previous case.
		- using hardware, it's guaranteed that if a process is currently performing a test-and-set, no other process may begin another test-and-set until the first process's test-and-set is finished. so the test-and-set function is
		non-preemptive.
		
		//P1
		do{
			while(TestAndSet(&lock));
				critical section
			*lock = FALSE;
				remainder section	
		} while (TRUE);
		
		- it satisfies mutual exclusion, but does not satisfy bounded waiting. 
		
	
	-> Turn Variable (Strict Alteration):
		- solution for 2 processes.
							P0:										P1:
							while (turn != 0);						while (turn != 1);	
							------ CS								------ CS
							turn=1;									turn=0;
		
		- ensures mutual exclusion and bounded waiting.
		- does not ensure progress. if turn = 0, and P1 wants to enter empty CS, it cannot until P0 runs and sets turn to 1. similarly, if P0 wants to enter the CS but turn is 1, then it cannot until P1 sets turn to 0. one process blocks the other even when the CS is empty, hence progress is not ensured.
		

-> SEMAPHORES:
	- techinque to manage concurrent processes (solve critical section problem) by using a simple non-negative integer value called as a semaphore.
	- shared between threads/processes.
	- S is an integer variable, accessed only through two standard atomic operations: wait() (P) and signal() (V).
	
	- Entry code: P(), Down, Wait
	- Exit code : V(), Up  , Signal, Post, Release
	
	wait() operation:				signal() operation:
	P (Semaphore S){				V (Semaphore S){
		while (S<=0) ;					S++;
		S--;						}
	}
	
	basically while S<0, some process is already in its critical section and this process has to wait.
	
	signal() called when the the process in critical section completes its operation.
	
	- S is shared between processes. when one process is modifying S, no other process can modify S (executed indivisibly).
	
	1) Binary Semaphore:
		- value ranges between 0 and 1. aka Mutex Locks (provide mutual exclusion). S initially is 1.
		- S=0, some process is already in its critical section and the requesting process has to wait.
		- S=1, requesting process is free to enter the critical section or use the shared resource.
		
		- this implementation allows for mutual exclusion without busy waiting.
		Down(Semaphore S){
			if (S->value == 1) {
				S->value = 0;	
			} else {
				block this process and add in S->list;
				sleep();
			}
			
		Up (Semaphore S){
			if (S->list isEmpty){
				S->value = 1; //no process waiting, the last process executed in CS. now any other process can directly come into the CS withuot going into the waiting list.
			} else {
				select a process from S->list;
				wakeup();
			}
		}
		
		- it is important to note that when a process wakes up, it does not execute the Down part again. it goes into the CS if it can.
	
	2) Counting Semaphore:
		- value can range over an unrestricted domain. used to control access to a resource that has multiple instances.
		- set the initial value of semaphore to the number of instances of the shared resource. (let x)
		- so, while S > 0, x number of processes can access the shared resource, after which any further process gets trapped in the while loop until a resource becomes free.
		
	-> disadvantages of semaphores:
		1) requires busy waiting. any process not in critical section must loop continuously in the entry code. it wastes CPU cycles (unproductive). called as a spinlock (process "spins" waiting for the lock)
		this can be solved by: rather than engaging in busy waiting, the process can block itself. the block opeartion places a process into a waiting queue associated with the semaphore, and the process is now in waiting state. then, control is transferred to the CPU scheduler which selects another process to execute.
		
		2) that also has problems. may result in DEADLOCKs, where two or more processes are waiting indefinitely for an event that can be caused only by one of the waiting processes.
		

	# Solution of Producer-Consumer Problem using Semaphores:
		- uses Binary and counting semaphores. the counting semaphores keep track of the empty and full slots in the buffer, and the binary semaphore ensures the mutual exclusion.
			COUNTING SEMAPHORES: Empty (initially N empty slots), Full (initially 0 full slots)
			BINARY SEMAPHORE: S = 1;
			
		Produce_item (item p){
			Down(Empty)
			Down(S)
			----------
			Buffer[IN] = p
			IN = (IN+1)%N
			----------
			Up(S)
			Up(Full)
		}
		
		Consume_item (){
			Down(Full)
			Down(S)
			----------
			item c = Buffer[OUT]
			OUT = (OUT+1)%N
			----------
			Up(S)
			Up(Empty)
		}
		
		- this ensures mutual exclusion and also ensures synchronization (even when there's preemption) between the processes without busy waiting.
		}
		

	# Solution of Reader-Writer Problem using semaphores:
		- CRUD operations in a database. a reader and a writer cannot read/write on the same data simultaneously. two writers cannot write to the same locations simultaneously. two readers can read the database concurrently.
		
		- The semaphore mutex is used to ensure mutual exclusion when the variable read count is updated.
		- The semaphore db functions as a mutual exclusion semaphore for the writers. It is also used by the first or last reader that enters or exits the critical section. 
		- The rc variable keeps track of how many processes are currently reading the object.
		- two binary semaphores;
			Semaphore mutex = 1; to ensure mutual exclusion
			Semaphore db = 1; 
			int rc = 0; //keep track of number of current readers
			
			void Reader(){
				while (true) {
					down(mutex);
					rc = rc + 1;
					if (rc==1) down(db);
					up(mutex);
					-----------CS;
					down(mutex);
					rc=rc-1;
					if (rc==0) up(db);
					up(mutex);
				}
			}
			
			void Writer(){
				while (true) {
					down(db);
					-----------CS;
					up(db);
				}
			}
		
		- suppose one reader comes. he sets rc to 1 and db to 0. doing this will ensure no writer can enter (as db is set to 0). the down(mutex) and up(mutex) ensures mutual exclusion for different sections of the code, and ensures that preemption can only be valid if all the necessary code has been executed (like if first reader increments rc to 1 and gets preempted before up(mutex), then no other reader can enter till the first reader enteres, and so on).
		
		- only when the readers are decremented and reach a count 0 can a writer enter the db. after this, no reader or writer can enter until that writer exits.
		
		
	# Solution of Dining Philosophers Problem using Semaphores:
		- multiple philosophers seated round a table. every philosopher can either think or eat. there are two forks (one on left and one on right) for each philosopher. the philosopher can eat only if both the forks are available. so the philosopher can either TAKE the fork, or PUT the fork after eating.
		
		- this requires using an array of semaphores S to keep track of the availability of each fork. only when both the forks are avaiable can the philosopher enter the CS (i.e., eat). all are initially 1.
		
			void Philosopher(){
				while (true){
					think();
					wait(take_fork(S[i])); //pick the left fork
					wait(take_fork(S[(i+1)%N]); //pick the right fork
					------------CS
					signal(put_fork(S[i])); //return the left fork
					signal(put_fork(S[(i+1)%N]); //return the right fork
				}
			}
			
		- However, this code can still have a deadlock situation. assume that there are 5 philosophers. P0 picks F0, preempts, P1 picks F1, preempts, P2 picks F2, preempts, P3 picks F3, preempts, P4 picks F4. now if P4 wants to pick F0, it requires P0 to finish eating. but P0 can only finish if P1 does. and so on. hence, deadlock since all semaphores are 0 now but noone can proceed.
		
		solution? for any one Philosopher, the code can be changed: he will pick the RIGHT FORK first instead of the LEFT FORK. this will remove the deadlock, as now, P4 will not claim F4 before trying to claim F0. F0 makes it block, so now P3 can get F3 and F4, releasing F3 as well, and in that chain all semaphores are UPped and all philosophers can enter the CS one-by-one.
		
		
# DEADLOCKS:
	- if two or more processes are waiting for the occurrence of some event, which never happens, then we say these processes are in a deadlock. usually, this occurs when both are waiting for each other to occur but none can occur without the occurrence of the other.
	
	4 MAIN CONDITIONS FOR DEADLOCK TO OCCUR:
		1) Mutual Exclusion
		2) No Preemption: a process cannot preempt itself just so that another resource gets its held resouce and can complete itself, resulting in both processes to complete.
		3) Hold & Wait: process holding a resource and waiting for another resource.
		4) Circular Wait: a circular cycle of hold and wait of resources. one dependent on the other.
		
	- P1 ----> R1 => P1 requests for the resource R1
	  P1 <---- R1 => P1 has acquired (is holding) the resource R1 (R1 has been allocated to P1)
	  
	
	- RAG (Resource Allocation Graph) can be used to visually see if there is a deadlock or not. it basically draws out all the processes, resources held by them, and resources required by them. note: each resource may also have multiple instances (like registers)
	
	for SINGLE-INSTANCE RESOURCES ONLY in an RAG
	- if there is a cycle, then there will definitely be a deadlock. 
	  if RAG has no cycle, then no deadlock (for single instance, direct dekhke bata sakte hai)
	  eg: it is possible that there is a circular wait but no deadlock if there is even one multi-instance resource.
	
	- initial available resources should be found from the number of instance of each resource minus the number of instances of that resource that are already acquired by some process. this forms a tuple, giving the availability for each resource. use this tuple to see if any process can be completed. if it can, its acquired resources will get freed and added to the tuple, and continue like this.
	
	- the table should have a row for each process, and 2 columns: one for the allocation, containing columns of each resource and the number of instances allocated for each resource would be its value. one for the request.
	
	- FINITE WAITING FOR A RESOURCE => STARVATION, INFINITE WAITING => DEADLOCK
	
	
	-> Methods to handle Deadlocks:
		1) Deadlock ignorance (Ostrich method):
			- very widely used. just ignore the deadlocks.
			- occurs very rarely, so it's not optimal to write full-fledged code to handle deadlocks. incurs overhead.

		2) Deadlock prevention:
			- ensure the four conditions do not occur. (atleast one of them should be violated)
			- eg: if resources can be shared (violating mutual exclusion), then deadlocks can be prevented.
			- eg: if hold-and-wait is there, preempt one process so other gets executed, after which this gets executed. violating no preemption rule can also prevent deadlocks.
			
		3) Deadlock avoidance (Banker's Algorithm):
			- Banker's Algorithm (checking safe/unsafe situation) (safe sequeunce => no deadlock for this sequence, unsafe => deadlock)
			- require info before hand about the resource requirements of each process, and the resources allocated to each resource.
			- 2 columns: allocated and max need. this gives us the remaining resources as a 3rd column. then from the initial available resources (like in RAG), we check if there's a way to avoid deadlock such that the resource requirements of each process are met.
			
		4) Deadlock detection & recovery:
			- Detect in advance whether deadlock can occur using RAG.
			- harder to do so as it's not easy to predict the resources required by processes.
			- recovery by killing the processes that are in deadlock, but can hamper the processes. we can also preempt the deadlocked processes/resources.
			
			
	- total number of resources shared + total number of processes > total demand , to always avoid deadlocks.
	
	
# MEMORY MANAGEMENT:
	- manage the various kinds of storage like RAM, hard disk, cache, registers, etc.
	
	- main focus on primary memory or RAM.
	
	- CPU is connected to RAM, not secondary memory. hence, the programs that are stored in Secondary Memory need to be brought inside RAM to be run by the CPU. this can be done with the help of the OS.
	
	- Bringing many programs in RAM => increase degree of multiprogramming, this increases CPU utilization. multiprogramming can be increased by increasing the size of RAM or increasing the number of processes in RAM.
	
	- basically the RAM is divided into partitions or blocks. the way that the processes are assigned to these blocks decides is determined by the type of memory management technique used.
	
	- types of Memory Management Techniques:
		1) Contiguous: Fixed Partition (Static), Variable Partition (Dynamic) 
		2) Non-Contiguous: Paging, Multilevel Paging, Inverted Paging, Segmentation, Segmented Paging
		
	1) Contiguous:
		- spanning of processes across blocks is not allowed. so even if a process is of 8mb, it cannot take up two 4mb blocks. either the big process is put into a big block, or small processes are brought into memory.
		
		a) Fixed Partitioning:
			- No. of partitions is fixed.
			- Size of each partition may/may not be the same, but the number is constant.
			- eg: partitions like 4,8,8,16 or 8,8,8,8
			- if i put a 2mb process in a 4mb block, the remaining 2mb is wasted. it cannot be utilised by any other process. this is called INTERNAL FRAGMENTATION, when entire space within a block cannot be utilised. an entire block/partition is assigned to a single process.
			- the partition sizes are configured initially when designing the system. these cannot be changed.
			- no. of processes and max size of process is fixed. limits degree of multiprogramming.
			- combined waste of space due to internal fragmentation in the different blocks is called EXTERNAL FRAGMENTATION. so even if first block has 2mb waste, second has 5, third has 10, fourth has 4, we cannot accomodate even a 1mb process if all the blocks are occupied by some process. 
			
		b) Variable Partitioning:
			- no. of partitions are not fixed, and sizes can also vary like in fixed.
			- hence, no limitation on the number of processes (only limited by the size of the RAM)
			- hence, degree of multiprogramming is much better than fixed partitioning. 
			- Space is allocated depending on the size of the process at run-time.
			- hence, no scope for internal fragmentation here as the required space is allocated to each process, not more, not less.
			- however, if there are holes created (like 2 non-contiguous processes are terminated), and some process with size bigger than each of these arrives, it cannot be accomodated as in contiguous allocation, the process cannot span multiple blocks. hence, external fragmentation still occurs even if internal doesn't.
			- one solution to external fragmentation is compaction, i.e., moving all the processes to one side and holes to the other. this is undesirable as these processes have to first be blocked to change its address space, and also incurs overhead. still, better than fixed as no internal fragmentation and higher degree of multiprogramming.
			- also, allocation/deallocation is bit complex, as there is no fixed no. of processes.
			
		-> Various allocation methods in contiguous memory allocation (both static and dynamic):
			1) First-Fit: Allocate the first hole that is big enough. simple and fast to implement. most convenient.
			2) Next-Fit: Same as above, start search from the last allocated hole (does not mean from the bottom, it means from the most recent allocated hole)
			3) Best-Fit: Allocate the smallest hole that can accomodate this process. searches the entire list of blocks to find the smallest hole. can lead to very small holes in the blocks, resulting in external fragmentation as all the blocks are almost nearly filled. it however minimises internal fragmentation.
			4) Worst-Fit: Allocate the biggest hole. this also requires searching of the entire list, hence slow. creates large holes so internal fragmentation will occur only when a large number of processes are accomodated.
			
			- generally, variable size partitioning performs best in Worst fit and worst in Best fit and Fixed size partitioning performs best in Best fit and worst in Worst fit
			
	2) Non-Contiguous:
		- a process can be spanned across multiple "blocks" in the RAM.
		- external fragmentation can be removed by this method, as there is no need of these process "blocks" to be arranged consecutively.
		- division of the process at run-time is costly. hence, these processes are divided into "pages" in the secondary memory itself, before being brought into RAM. similarly, the RAM is divided into "frames".
		
		a) Paging:
			- the important point is, FRAME SIZE = PAGE SIZE. so the different process pages can be placed in any frame, removing external fragmentation.
			- size of main memory (in bytes) indicates the number of bytes that can be stored in the main memory
			- the CPU demands for a specific byte from the process. we know that the process is divided into pages. the CPU generates a LOGICAL ADDRESS specifying the page no. and the byte (page offset). this is mapped to the actual PHYSICAL ADDRESS  (or ABSOLUTE ADDRESS) in the RAM by the MMU.
			- the binary value of the PAGE NO. and the PAGE OFFSET gives the specific byte that the CPU wants to access from the process. this page no. is then looked up in the PAGE TABLE by the MMU to identify which frame is it in, and the OFFSET within that frame (= page offset). this will then give the specific BYTE in the physical address.
			- no. of entries in the page table = no. of pages of the process. each entry denotes the frame no. for the ith page.
			- every page table entry is of this format:
			Frame No. bits, Valid(1)/Invalid(0), Perms (RWX), Referenced b4 (1/0), Caching (enable?), dirty/modify
				frame no. is mandatory, rest are optional.
			
		b) Two-level paging:
			- occurs when the size of the page table is more than the size of a frame. usually, the page table would fit inside a single frame and so there would be no need for two-level paging.
			- but when the logical address space is very large, a large page-table is required to map them to frames. this increases the size of the page table. if the page table cannot fit inside a single frame, this page table has to be divided into pages of its own to store it in the RAM. this is called two-level paging.
				here, page-table size = no. of pages * no. of bits to represent frames
			- now, accessing this page-table requires the use of another page-table. that page-table consists of mappings of the pages of the OG page-table to the RAM.
			- now, the logical address would have three parts: 
				OUTER PAGE TABLE NO. | OFFSET IN THAT OUTER PAGE (to get INNER PAGE) | OFFSET WITHIN INNER PAGE 
			- so now, in the RAM, we are storing the INNER PAGE TABLE into multiple pages (determined by the Outer page table). the first few bits give the page to determine which set of inner pages, next few bits give the frame of the specific page we require, final few bits give the offset within the frame and hence, the bit in the RAM)
			
		
		c) Inverted paging:
			- each process has its own page table, and these page tables have to be stored in the main memory. problem is that it may require a large number of frames just to store the page tables. to search for a particular page, we first have to get the page table, then the frame from that page table for that particular process.
			- in inverted paging, we have one GLOBAL page table of the form:
										fr.no | Page No | Process Id
			no of entries = no of frames
										
			- this uniquely identifies the page stored in each frame based on the process.
			- in normal paging, PageTable[pageIndex] = frameIndex
			- in inverted, PageTable[frameIndex] = {ProcessId, PageNo}
			- now, when the CPU generates a logical address, the frame number is found by checking each record of the page table, and seeing which frame has that page no. for that process. hence, higher search time.
			
		
	-> THRASHING:
		- upto a certain point, increasing the degree of multiprogramming increases the CPU utilization. however, after that point, the increase in the degree of multiprogramming results in a decrease in CPU utilization. This is called thrashing.
		
		- degree of multiprogramming can be increased by diversifying the pages of the processes present in MM. one page of each process as an example, not the entire process. but this can cause problems.
		
		- suppose that we have the first page of each process stored in memory. now, if we require page 2 of a process, that will not be found in the memory, resulting in a PAGE FAULT. this will result in a PAGE FAULT SERVICE TIME. now, the page is brought from the hard disk into the main memory which takes a lot of time. this is why increasing multiprogramming alot can decrease CPU utilization after one point (as most of the time is then utilized for servicing page faults and not running processes)
		
		- this can be solved by either increasing the size of the main memory, or using a Long Term Scheduler to not have many processes in the RAM
		
		-> Page Fault Handling Routine-
			- The following sequence of events take place-

			1) The currently running process is stopped and context switching occurs.
			2) The referenced page is copied from the secondary memory to the main memory.
			3) If the main memory is already full, a page is replaced to create a room for the referenced page.
			4) After copying the referenced page successfully in the main memory, the page table is updated.
			5) When the execution of process is resumed, step-02 repeats.
			
		EAT (without page faults) = Hit ratio of TLB * (Access Time of TLB + Access time of RAM) + Miss ratio of TLB * (Access Time of TLB + (L+1)*Access time of RAM), where L = no. of levels of page table.
		
		EAT (with page faults) = Page fault rate * (EAT without page fault + page fault service time) + (1 - page fault rate) * (EAT without page fault)
		
		
	-> SEGMENTATION:
		- similar to paging, process is divided into segments that are stored in the main memory.
		
		- paging simply divides the process into pages without knowing the content or connection between the pages. but segmentation works on user POV (like segmenting based on functions like add, multiply, etc.)
		
		- segmentation creates segments according to the user POV. each page has the same size in paging, but each segment may not have the same size.
		
		- these segments are directly stored in the MM without something like frames, as segment size varies. there is also a segment table that indicates which segment is present at which BASE ADDRESS (start of the segment), and the size that the segment takes up from that base.
		
		- the logical address in this case would be SEGMENT NO. | SEGMENT SIZE/OFFSET . it is important that the segment size/offset that the CPU is demanding for a segment be <= the size of the segment in the table. otherwise, a trap is generated. so the CPU can only access a segment or its part and not extend one part of the segment to another. it cannot access part of S1 and part of S2.
		
	-> OVERLAY:
		- a method by which a large-size process can be put into the main memory, even if the process has a larger size than the main memory itself.
		
		- a process may be divided into several parts, and each functionality is executed in the order they should (being swapped in and swapped out). this should however be done by the user as the OS does not have any such drvier by default. this is particularly useful in embedded systems where there is a fixed type of functionality.
		
		- the divisions of the process should be independent. no one division should depend on the code/data in another.
		
	-> VIRTUAL MEMORY:
		- provides an illusion to the user that a process whose size is larger than the main memory can also be executed despite the main memory having a finite size.
		
		- it also helps in increasing the degree of multiprogramming in a similar way, by giving the illusion that there is more RAM available.
		
		- multiple processes are each divided into pages (like in paging), and only the required pages of each process are brought into the memory at a given time. (maybe by locality of reference). this is achieved by swap in/swap out operations, using which the pages are moved between the hard disk and the main memory. this is called page replacement.
		
		- if the CPU requires a page of a process that is not in the RAM, then it causes a page fault by generating a trap. when the trap is generated, the control switches from the user to the OS (context switching)
		- OS checks if the user is authenticated to access the page that he is demanding.
		- this page currently resides in the hard disk (in LAS). OS locates that specific page for that process, and brings it to some empty spot in the MM.
		- that record is then updated in the page table so that further access to that page does not cause page fault.
		- control is returned back to the user.
		
		hence, EMAT = p * (page fault service time (usually ms) + MA) + (1-p) * (main memory access time (MA) (usually in ns))
				where,  EMAT = Effective Memory Access Time
						p = prob. of page fault occurring.
						
		
	-> TRANSLATION LOOKASIDE BUFFER (TLB):
		- first the normal paging part is achieved, using which pages of processes are loaded into frames, and a page table contains the records of which pages are in which frames.
		
		- problem is that the page table also resides in the main memory just like the pages. so for accessing each page, the CPU first has to retrieve the page table every time, which can be costly. so, x time for accessing MM for page table, and another x time for accessing the frame once the page table has been read. => total 2x time.
		
		- if page table is large (multilevel), then time increases even more. this time can be reduced by using a faster memory to store the page table; cache (TLB)
		
		- TLB faster than RAM. so page table entries are stored in TLB. so first, when a CPU generates a logical address, it is searched for in the TLB (by matching with tags). if TLB hit (found in TLB), then the page is accessed using the frame obtained in the TLB. if TLB miss, then normal procedure (of finding in page table, and then accessing the frame) and the TLB is updated with the frame.
		
		=> EMAT = Hit-Ratio * (TLB + x) + Miss-Ratio * (TLB + 2x) (ASSUMING NO PAGE FAULT OCCURS)
		
		- if page fault can occur, then the miss case would have to include the time for accessing the page from the harddisk as well (page fault service time)
		
	-> PAGE REPLACEMENT ALGORITHMS:
		- 3 main algos to ensure virtualization of memory and increase degree of multiprogramming
		- if there is empty space in the memory first that is filled irrespective of the algorithm
		- input is a reference string consisting of the pages given as inputs one after the other indicating how they are to be accessed.
		1) FIFO:
			- the first frame to get filled is replaced incase there is a page miss
			- if page hit, then do nothing.
			- Belady's Anomaly: generally, increasing the no. of frames (or the memory available) to store processes should increase the degree of multiprogramming and CPU utilisation by decreasing the miss rates (since more no. of pages are now present in memory). but sometimes, increasing the no. of frames DECREASES the hit rate in FIFO or INCREASES the no. of page faults. this is called Belady's Anomaly.
			
		2) Optimal Page Replacement:
			- based on FUTURE
			- replace the page which is not used in longest dimension of time in future.
			- basically replace that page in the currently filled memory who's first demand in the future is the latest.
			
		3) LRU (Least Recently Used):
			- based on PAST
			- replace that page in the filled memory that has not been accessed for the longest period.
			- so, the last access to a page in the memory was long ago. replace that page with the new page
			
		4) MRU (Most Recently Used):
			- based on PAST
			- replace that page from the main memory that has been accessed most recently.
			
			
# HARD DISK ARCHITECTURE:

								platters --> surfaces --> tracks --> sectors --> data
	
	- platters are disk-like structures containing 2 surfaces. these platters are placed one above the other connected by a spindle that can spin uni-directionally. there is a read-write head on each of these platters (the head has a pointer on the above surface and below surface as well, something like:
														 __________
										----------------|__________ platter1
										|				 __________
										----------------|__________ platter2
										|				 __________
										----------------|__________ platter3
	- these r/w heads are connected by an actuator arm. these heads can move either front-or-back.
	
	- each surface has multiple concentric rings called tracks. the tracks contain sectors. the sectors contain data, which is accessed by the forward/backward movement of the head and the rotation of the platters.
	
	- disk size = no.of platters * surfaces per platter * tracks per surface * sectors per track * data per sector
	
	-> DISK ACCESS TIME:
		1) Seek Time = Avg. time taken by R/W head to read desired track
		2) Rotation Time = Time taken for one full rotation of a platter
		3) Rotational Latency = Time take to reach to desired sector ( = 0.5 * Rotational Time )
		4) Transfer time = Data to be transfered / Transfer Rate
		5) if transfer rate not given, 
			Transfer Rate (Data Rate) = No.of Heads (Surfaces) * Data in one track * No of rotations per second
			
			ACCESS TIME = SEEK TIME + ROTATIONAL LATENCY + TRANSFER RATE [+CONTROLLER TIME if given] [+QUEUE TIME]
			
		
	-> DISK SCHEDULING ALGORITHM:
		- goal is to reduce the seek time.
		- various algos like FCFS, SSTF (Shortest Seek Time First), SCAN, LOOK, CSCAN (Circular SCAN), CLOOK (Circular Look)
		
		1) FCFS:
			- we are given a sequence of tracks that are to be read from the queue, and the beginning position of the head (initially which track it is on).
			- now, we can find the number of track movements by considering the sequence in order. say initially head is on track 50. then queue gives track 82 and 30. then track movements = (82 - 50) + (82 - 30). the direction of head changes once (from forward to backward)
			- no starvation, as all tracks are processed in the order of the sequence.
			- performance is low however, since it's brute force. main objective would be to reduce the number of direction changes (ideally one forward pass and one backward pass should do it)
			
		2) SSTF:
			- tracks from the queue are chosen so that it takes shortest time to go to a track from the current track (not the shortest absolute value of a track, min(|currTrack - tracks[i]|)
			- once we reached to that track which was closest to initial track, we repeat the process.
			- this will ensure that direction change takes place only when required (tries minimizing it)
			- avg. response time is good
			- starvation is a problem. it cannot be guaranteed when one request will be processed
			- overhead is generated to find the closest track from each track
			
		3) SCAN (aka Elevator Algorithm):
			- requires initial direction to be specified
			- move in one direction and move till the VERY END (even if the last track in that direction is processed, it goes till the very end), and then change direction, and go till only the required track in this direction (not till the very end). 
			- so only once direction is changed.
			- the track movements are considered till the very end for the first direction, and from that end to the required track on the other direction.
			- goes till the very end to account for dynamic addition of requests in that direction. but once direction is changed and the request is added "behind" it, it will finish in that changed direction, and then go back to that request in the OG direction. this can lead to starvation.
			
		4) LOOK:
			- requires initial direction to be specified
			- almost same as SCAN, but does not go till the end of the first direction. just goes to the required track in both the directions.
			
		5) CSCAN:
			- requires initial direction to be specified
			- first direction, same as SCAN (go till the very end)
			- when direction changed, the head goes all the way till the end of the other direction without reading any track or processing any track requests.
			- then it changes direction on that other end, and starts processing the tracks.
			- the tracks traversed from the end of one dirn to the other end must also be added in the total number of tracks traversed.
			
		6) CLOOK:
			- requires initial direction to be specified
			- combination of CSCAN and LOOK
			- the head goes in the first direction upto the last track request (and not till the very end). from there it changes direction, and goes all the way back till the last requested track on the opposite direction (not till the very end like in CSCAN). no requests are fulfilled during this movement. then it changes direction there, and starts processing the track requests.
			- the movement from the last request in one direction to the last request in the other direction must also be counted for. 
			

# FILE SYSTEM:
	- File attributes are: (metadata)
		1) Name
		2) Extension (type)
		3) Identifier (from the POV of the OS)
		4) Location
		5) Size
		6) Modified date, Created date
		7) Protection/Permissions
		8) Encryption, Compression
		
	- different file allocation methods are:
		1) Contiguous Allocation
		2) Non-contiguous Allocation : Linked List, Indexed
		- this allows for efficient disk utilization.
		- allows to optimise the way the files are accessed from the disk.
		
		1) Contiguous:
			- file blocks are stored contiguously in the disk sectors.
			- not needed to start from the initial sector. can start from any sector but all sectors continuous.
			- file directory stores all information about the stored files. file, its start position, and its length (no. of sectors it occupies)
			
			-> Advantages:
				- easy to implement
				- excellent read performance, since all sectors located side-by-side on the same track
				
			-> Disadvantages:
				- fragmentation in disk (both internal and external, internal can occur in all cases though)
				- difficult to grow a file (if no contiguous location available to store edited info)
				
		2) Non-contiguous:
			- the blocks may be stored on any sector in the disk, not necessarily contiguously
			- these random locations for a file are however mapped somehow.
			
			a) Linked-List Allocation:
				- each file has a start position. the block at this position is then connected to the next block which can reside anywhere, and so on like a linked list.
				
				-> Advantages:
					- no external fragmentation, since any random block can be utilised if there are free blocks
					- file size can be increased and updated easily by adding new blocks to the linked list.
					
				-> Disadvantages:
					- large seek time, as the sectors may be located on differing tracks as well.
					- random/direct access is difficult, since getting a block in the middle requires the traversal of the entire linked list. we cannot directly access it like we can if it were in an array
					- overhead of maintaining pointers.
					
			b) Indexed-List Allocation:
				- the blocks are stored randomly again, and one block contains an array mapping which block exists in which sector.
				- this way, direct/random access is easier as they are mapped in an array stored on some sector.
				- now in the directory, we just store the file and the sector in which this indexed-array exists cuz using this array we can access the other blocks
				
				-> Advantages:
					- supports direct access
					- no external fragmentation
					
				-> Disadvantages:
					- Overhead of pointers (maintain a huge number of pointers pointing to each block)
					- Multilevel Index (if index table is large, the index table itself might require an index table of its own like multi-level paging)
					
				eg: UNIX supports INODE (I stands for Index) file structure.
					- if index-table becomes large, it's harder to store the table in one single block. in INODE, each data block consists of the following:
					1) attributes
					2) direct blocks (pointers that directly map to blocks of the file)
					3) single indirect blocks (a pointer to another block, that points to other blocks of the file)
					4) double indirect (2 level ke baad data blocks milenge)
					5) triple indirect (3 levels)
					
					
					
					
					
======================================================================================================================
********************************************* OPERATING SYSTEM NOTES *************************************************
======================================================================================================================
